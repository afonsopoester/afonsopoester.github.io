[
  {
    "objectID": "posts/post_6/post_6.html",
    "href": "posts/post_6/post_6.html",
    "title": "Predcting number of points of NBA players in the regular season",
    "section": "",
    "text": "Hello, this time, I will try to develop a regression model with Tidymodels, a framework I’ve been studying recently. We will analyse NBA data from Kaggle. I developed much of this code adapting blog posts from Julia Silge, which have been helping me to understand the initial steps to tidy modelling.\n           \n\nlibrary(tidyverse)\nlibrary(tidymodels)\n\n## NBA ----\n\n## Predict the number of points of a player in the regular season \n\n\nnba &lt;- read_csv(\"nba.csv\")\n\nnba_f &lt;- nba %&gt;%\n  filter(Season_type == \"Regular%20Season\") %&gt;%\n  select(\n    PTS, year, PLAYER_ID, TEAM_ID, GP, MIN, FG_PCT, FG3_PCT, FT_PCT, OREB, DREB, AST, STL, BLK, TOV, PF\n  ) %&gt;%\n  mutate(\n    PLAYER_ID = as.character(PLAYER_ID),\n    TEAM_ID = as.character(TEAM_ID),\n    year = as.numeric(substr(year, 1, 4))\n  ) \n\nglimpse(nba_f)\n\nRows: 6,259\nColumns: 16\n$ PTS       &lt;dbl&gt; 2280, 2133, 2036, 2023, 1920, 1903, 1786, 1577, 1562, 1560, …\n$ year      &lt;dbl&gt; 2012, 2012, 2012, 2012, 2012, 2012, 2012, 2012, 2012, 2012, …\n$ PLAYER_ID &lt;chr&gt; \"201142\", \"977\", \"2544\", \"201935\", \"2546\", \"201566\", \"201939…\n$ TEAM_ID   &lt;chr&gt; \"1610612760\", \"1610612747\", \"1610612748\", \"1610612745\", \"161…\n$ GP        &lt;dbl&gt; 81, 78, 76, 78, 67, 82, 78, 82, 82, 74, 82, 78, 69, 79, 82, …\n$ MIN       &lt;dbl&gt; 3119, 3013, 2877, 2985, 2482, 2861, 2983, 3076, 3167, 2790, …\n$ FG_PCT    &lt;dbl&gt; 0.510, 0.463, 0.565, 0.438, 0.449, 0.438, 0.451, 0.416, 0.42…\n$ FG3_PCT   &lt;dbl&gt; 0.416, 0.324, 0.406, 0.368, 0.379, 0.323, 0.453, 0.287, 0.36…\n$ FT_PCT    &lt;dbl&gt; 0.905, 0.839, 0.753, 0.851, 0.830, 0.800, 0.900, 0.773, 0.84…\n$ OREB      &lt;dbl&gt; 46, 66, 97, 62, 134, 111, 59, 45, 42, 175, 48, 29, 86, 218, …\n$ DREB      &lt;dbl&gt; 594, 367, 513, 317, 326, 317, 255, 271, 215, 495, 272, 203, …\n$ AST       &lt;dbl&gt; 374, 469, 551, 455, 171, 607, 539, 496, 531, 192, 204, 604, …\n$ STL       &lt;dbl&gt; 116, 106, 129, 142, 52, 145, 126, 169, 74, 62, 76, 75, 128, …\n$ BLK       &lt;dbl&gt; 105, 25, 67, 38, 32, 24, 12, 36, 19, 91, 24, 30, 56, 22, 31,…\n$ TOV       &lt;dbl&gt; 280, 287, 226, 295, 175, 273, 240, 254, 243, 143, 151, 218, …\n$ PF        &lt;dbl&gt; 143, 173, 110, 178, 205, 189, 198, 164, 172, 187, 173, 194, …\n\n\n                       \nFirst step: use tidymodels functions to separate the train and test datasets and create a vfold object.\n\n## Data Split ----\n\nset.seed(502)\nnba_split &lt;- initial_split(nba_f, prop = 0.80, strata = PTS)\nnba_train &lt;- training(nba_split)\nnba_test  &lt;-  testing(nba_split)\n\nnba_folds &lt;- nba_train %&gt;%\n  vfold_cv(v = 5, repeats = 1, strata = PTS)\n\n                       \nNext, we begin to build a recipe, begining with the formula. The variable PLAYER_ID was set as an ID. Finally, we normalize numeric variables and encode categorical variables.\n\n## Model formula ----\nform &lt;- as.formula(paste(\"PTS\",\" ~ \", \".\"))\n\n## Model recipe ----\n\nmod_recipe &lt;- recipe(formula = form, data = nba_train) %&gt;%\n  update_role(PLAYER_ID, new_role = \"id\") %&gt;%\n  step_normalize(all_numeric_predictors()) %&gt;%\n  step_dummy(all_predictors(), -all_numeric(), one_hot = F)\n\nmod_recipe_prep &lt;- prep(mod_recipe, retain = T)\nmod_recipe_prep\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:    1\npredictor: 14\nid:         1\n\n\n\n\n\n── Training information \n\n\nTraining data contained 5006 data points and no incomplete rows.\n\n\n\n\n\n── Operations \n\n\n• Centering and scaling for: year, GP, MIN, FG_PCT, FG3_PCT, ... | Trained\n\n\n• Dummy variables from: TEAM_ID | Trained\n\n\n               \nLet’s look how the data is transformed when the recipe is applied:\n\nmod_recipe_prep %&gt;% bake(new_data = NULL) %&gt;% glimpse()\n\nRows: 5,006\nColumns: 44\n$ year                &lt;dbl&gt; -1.671321, -1.671321, -1.671321, -1.671321, -1.671…\n$ PLAYER_ID           &lt;fct&gt; 101179, 203093, 2562, 2554, 203104, 2248, 1894, 20…\n$ GP                  &lt;dbl&gt; -0.39049839, -1.18655440, -0.39049839, -1.18655440…\n$ MIN                 &lt;dbl&gt; -0.7267063, -1.0084401, -0.7050344, -0.8760011, -0…\n$ FG_PCT              &lt;dbl&gt; -1.0738170, 0.1319478, -0.8180487, -0.8728562, -0.…\n$ FG3_PCT             &lt;dbl&gt; -0.15512765, -0.11022703, 0.12710481, 0.21049167, …\n$ FT_PCT              &lt;dbl&gt; 0.02876769, 0.29362197, -2.48502473, -0.16174154, …\n$ OREB                &lt;dbl&gt; -0.632882005, -0.362763041, -0.740929591, -0.77694…\n$ DREB                &lt;dbl&gt; -0.8697689, -0.8271258, -0.7418397, -0.8697689, -0…\n$ AST                 &lt;dbl&gt; -0.2847410, -0.7418568, -0.6078746, -0.7891447, -0…\n$ STL                 &lt;dbl&gt; -0.2947645, -0.7415845, -0.3585959, -0.7415845, -0…\n$ BLK                 &lt;dbl&gt; -0.6878534, -0.1227314, -0.6172132, -0.5818930, -0…\n$ TOV                 &lt;dbl&gt; -0.4810592, -0.8367129, -0.8197770, -0.7859052, -0…\n$ PF                  &lt;dbl&gt; -0.47434503, -1.07983655, -0.50461961, -0.89818910…\n$ PTS                 &lt;dbl&gt; 105, 104, 103, 100, 99, 96, 95, 95, 93, 91, 88, 87…\n$ TEAM_ID_X1610612738 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ TEAM_ID_X1610612739 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ TEAM_ID_X1610612740 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1,…\n$ TEAM_ID_X1610612741 &lt;dbl&gt; 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ TEAM_ID_X1610612742 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ TEAM_ID_X1610612743 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ TEAM_ID_X1610612744 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ TEAM_ID_X1610612745 &lt;dbl&gt; 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ TEAM_ID_X1610612746 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ TEAM_ID_X1610612747 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ TEAM_ID_X1610612748 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,…\n$ TEAM_ID_X1610612749 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ TEAM_ID_X1610612750 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ TEAM_ID_X1610612751 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,…\n$ TEAM_ID_X1610612752 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ TEAM_ID_X1610612753 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ TEAM_ID_X1610612754 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ TEAM_ID_X1610612755 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ TEAM_ID_X1610612756 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,…\n$ TEAM_ID_X1610612757 &lt;dbl&gt; 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ TEAM_ID_X1610612758 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ TEAM_ID_X1610612759 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ TEAM_ID_X1610612760 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,…\n$ TEAM_ID_X1610612761 &lt;dbl&gt; 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ TEAM_ID_X1610612762 &lt;dbl&gt; 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ TEAM_ID_X1610612763 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,…\n$ TEAM_ID_X1610612764 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ TEAM_ID_X1610612765 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,…\n$ TEAM_ID_X1610612766 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n\n\n                       \nThen, we define the LASSO regression model with the “glmnet” package and add it to a workflow object, along with the recipe. We also create a tunning grid, which will have the parameter “penalty” as the parameter we want to tune when we fit the first model.\n\n## Define model ----\n\nreg_model &lt;- linear_reg(penalty = tune(), mixture = 1) %&gt;%\n  set_engine(\"glmnet\")\n\n## Grid ----\nlambda_grid &lt;- grid_regular(penalty(), levels = 50)\n\n## Start workflow ----\n\nreg_wf &lt;- \n  workflow() %&gt;%\n  add_model(reg_model)  %&gt;%\n  add_recipe(mod_recipe)\nreg_wf\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n2 Recipe Steps\n\n• step_normalize()\n• step_dummy()\n\n── Model ───────────────────────────────────────────────────────────────────────\nLinear Regression Model Specification (regression)\n\nMain Arguments:\n  penalty = tune()\n  mixture = 1\n\nComputational engine: glmnet \n\n\n                       \nNow, we fit the model with the k fold cross validation scheme defined earlier. The tune_grid() function receives the workflow, the ressampling scheme and the grid for the lambda parameter.\n\n## Fit with Tune Grid ----\nlasso_grid &lt;- tune_grid(\n  reg_wf, \n  resamples = nba_folds,\n  grid = lambda_grid,  \n  metrics = metric_set(rmse, mae, rsq)\n)\nlasso_grid\n\n# Tuning results\n# 5-fold cross-validation using stratification \n# A tibble: 5 × 4\n  splits              id    .metrics           .notes          \n  &lt;list&gt;              &lt;chr&gt; &lt;list&gt;             &lt;list&gt;          \n1 &lt;split [4003/1003]&gt; Fold1 &lt;tibble [150 × 5]&gt; &lt;tibble [0 × 3]&gt;\n2 &lt;split [4004/1002]&gt; Fold2 &lt;tibble [150 × 5]&gt; &lt;tibble [0 × 3]&gt;\n3 &lt;split [4005/1001]&gt; Fold3 &lt;tibble [150 × 5]&gt; &lt;tibble [0 × 3]&gt;\n4 &lt;split [4006/1000]&gt; Fold4 &lt;tibble [150 × 5]&gt; &lt;tibble [0 × 3]&gt;\n5 &lt;split [4006/1000]&gt; Fold5 &lt;tibble [150 × 5]&gt; &lt;tibble [0 × 3]&gt;\n\n\n                       \nFor each fold, the tune grid object holds the metrics obtained and we can plot the metrics considering the varible penalty.\n\n## Metrics ----\n\nlasso_grid %&gt;%\n  collect_metrics() %&gt;%\n  ggplot(aes(penalty, mean, color = .metric)) +\n  geom_errorbar(aes(ymin = mean - std_err, ymax = mean + std_err), alpha = 0.5) +\n  geom_line(size = 1.5) +\n  facet_wrap(~.metric, scales = \"free\") +\n  scale_x_log10() +\n  theme(legend.position = \"none\") +\n  theme_bw()\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\n\n                       \nThe performance of the model seems to get very slightly better with the LASSO penalty. So, we pull the best model from the tune grid object, based on the RMSE. The finalize_workflow() function unites the original workflow with the best model.\n\nlowest_rmse &lt;- lasso_grid %&gt;%\n  select_best(metric = \"rmse\")\n\nfinal_lasso &lt;- finalize_workflow(\n  reg_wf,\n  lowest_rmse\n)\n\nfinal_lasso\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n2 Recipe Steps\n\n• step_normalize()\n• step_dummy()\n\n── Model ───────────────────────────────────────────────────────────────────────\nLinear Regression Model Specification (regression)\n\nMain Arguments:\n  penalty = 1e-10\n  mixture = 1\n\nComputational engine: glmnet \n\n\n                       \nWe then apply the model one last time to the training and testing data, using the last_fit() function.\n\nlast_fit_lasso &lt;- last_fit(\n  final_lasso,\n  nba_split\n) %&gt;%\n  collect_metrics()\n\nlast_fit_lasso\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard     126.    Preprocessor1_Model1\n2 rsq     standard       0.925 Preprocessor1_Model1"
  },
  {
    "objectID": "posts/post_4/post_4.html",
    "href": "posts/post_4/post_4.html",
    "title": "How do athletes perform in the next Olympics Edition after winning their first Gold Medal?",
    "section": "",
    "text": "A Gold Medal in the Olympics is probably the highest an athlete can dream. While watching the 2024 Paris Olympic Games, and looking at some historic data, I tried to ask myself some questions that could be answered through creative data analysis. I thought about some medalists performance throughout multiple Olympic cicles, and asked myself: When athletes win their first Gold Medal, how do they perform in the next Olympics Edition? How often do they keep on the podium? Or do they even compete another edition?\nI tried to get some insight using data from Olympedia, downloaded from the brazilian project Base dos Dados.\nFirst things first: we read the data as CSV, extract the year the edition happened, and replace NAs in the Medal column as “No_Medal”. I also ordered the Medal column.\n\nlibrary(tidyverse)\nlibrary(showtext)\nlibrary(rstatix)\nfont_add_google(\"Commissioner\", \"Commissioner\")\nshowtext_auto()\n\nathlete_result &lt;- read_csv(\"athlete.csv\") %&gt;%\n  mutate(\n    edition_year = as.numeric(substr(edition, 0, 4))\n  ) %&gt;%\n  replace_na(list(medal = \"No_Medal\"))\nathlete_result$medal &lt;- factor(athlete_result$medal, levels = c(\"No_Medal\", \"Bronze\", \"Silver\", \"Gold\"), ordered = TRUE) \n\nFurther on, I filtered the data to have only the Summer Olympics edition, and made a dataset with the first time some athlete won the first gold medal of its career, and also added a value called “Next Edition”, to later on join with the respective result of the next edition. I considered the two occasion when the Olympics where not held 4 years after the previous one: 1920 and 1948.\n\nfirst_gold &lt;- athlete_result %&gt;%\n  filter(grepl(\"Summer\", edition)) %&gt;%\n  filter(edition_year != 2020) %&gt;%\n  select(edition, edition_year, edition_id, country_noc, sport, athlete, athlete_id, medal, position) %&gt;%\n  filter(medal == \"Gold\") %&gt;%\n  group_by(athlete_id) %&gt;%\n  slice_min(edition_year) %&gt;%\n  unique() %&gt;%\n  mutate(\n    next_ed = case_when(\n      edition_year == 1912 ~ 1920,\n      edition_year == 1936 ~ 1948,\n      .default = edition_year + 4\n    )\n  )\n\nFinally, I changed the original dataset, calling the “Medal” column “Next Medal”, so I could join that column with the one of the “First Gold” dataset.\n\nnext_edition_medals &lt;- athlete_result %&gt;%\n  select(edition, edition_year, edition_id, country_noc, sport, athlete, athlete_id, medal, position) %&gt;%\n  subset(athlete_id %in% first_gold$athlete_id)  %&gt;%\n  mutate(next_ed = edition_year) %&gt;%\n  select(next_ed, athlete_id, medal) \n\njoin &lt;- first_gold %&gt;%\n  left_join(next_edition_medals, by = c(\"athlete_id\",\"next_ed\")) %&gt;%\n  group_by(athlete_id) %&gt;%\n  slice_max(medal.y) %&gt;%\n  unique() %&gt;%\n  ungroup() \n\njoin$medal.y &lt;- addNA(join$medal.y)\nlevels(join$medal.y) &lt;- c(\"No Medal\", \"Bronze\", \"Silver\", \"Gold\", \"Did Not Participate\")\n\ndf &lt;- join %&gt;%\n  rename(\"first_gold\" = medal.x, \"next_medal\" = medal.y)\n\nNow the dataset summarises the first time an athlete won a Gold Medal, and then the next edition the athlete participated, and the best result obtained - since some athletes win way more than one medal in one edition. One thing I did not control, however, is whether the athlete won a medal in the same category, or another.\nAnd now to the first plot, I created a Waffle chart, representing what athletes get after winning their first Gold Medals. The squares represent every 100 athletes. The plot on the top shows the general result, counting the athletes who did not participate in the next ediiton of the Olympics. The plot below, ignores those athletes.\n\nlibrary(waffle)\none_a  &lt;- df %&gt;%\n  group_by(next_medal) %&gt;%\n  summarise(\n    n = n()/100\n  ) %&gt;%\n  ggplot(aes(fill = next_medal, values = n)) +\n    geom_waffle(colour = \"white\", n_rows = 8, size = 2, flip = TRUE) +\n    scale_fill_manual(values = c(\"purple\", \"#A77044\", \"#D7D7D7\", \"#FEE101\", \"red\"))  +\n    labs(\n      fill = \"Best result after first Gold Medal\"\n    ) +\n  theme_void() +\n  coord_equal() +\n  theme(legend.text = element_text(size = 16),\n        legend.title = element_text(size = 18))\n\none_b  &lt;- df %&gt;%\n  filter(next_medal != \"Did Not Participate\") %&gt;%\n  group_by(next_medal) %&gt;%\n  summarise(\n    n = n()/100\n  ) %&gt;%\n  ggplot(aes(fill = next_medal, values = n)) +\n    geom_waffle(colour = \"white\", n_rows = 8, size = 2, flip = TRUE) +\n    scale_fill_manual(values = c(\"purple\", \"#A77044\", \"#D7D7D7\", \"#FEE101\"))  +\n    labs(\n      fill = \"Best result after first Gold Medal\"\n    ) +\n  theme_void() +\n  coord_equal() +\n  theme(legend.text = element_text(size = 16),\n        legend.title = element_text(size = 18))\n\nlibrary(patchwork)\n\ntitle_one &lt;- \"Best results of athletes in the next Olympics edition after their first Gold Medal\"\ncaption_one &lt;- \"Source: Olympedia\"\n \nplot &lt;- one_a / one_b \n\nplot +\n  plot_annotation(\n    title = title_one,\n    caption = caption_one,\n    theme = theme(\n      plot.title = element_text(family = \"Commissioner\", size = 21),\n      plot.caption = element_text(family = \"Commissioner\", 30)\n   )\n  ) \n\n\n\n\n\n\n\n\nSince I’m Brazilian, I also wanted to look to the familiar faces of Brazilian gold medalists. Here, I did not divide the number of athletes by then so each square represents one athlete.\n\ntitle_two &lt;- \"Best results of brazilian athletes in the next Olympics edition after their first Gold Medal\"\ncaption_two &lt;- \"Source: Olympedia\"\n\ntwo &lt;- df %&gt;%\n  filter(next_medal != \"Did Not Participate\") %&gt;%\n  filter(country_noc == \"BRA\") %&gt;%\n  group_by(next_medal) %&gt;%\n  summarise(\n    n = n()\n  ) %&gt;%\n  ggplot(aes(fill = next_medal, values = n)) +\n    geom_waffle(colour = \"white\", n_rows = 8, size = 2, flip = TRUE) +\n    scale_fill_manual(values = c(\"purple\", \"#A77044\", \"#D7D7D7\", \"#FEE101\"))  +\n    labs(\n      fill = \"Best result after first Gold Medal\"\n    ) +\n  theme_void() +\n  theme(\n    legend.text = element_text(size = 16),\n    legend.title = element_text(size = 18)\n  ) + coord_equal()\n\ntwo + plot_annotation(\n    title = title_two,\n    caption = caption_two,\n    theme = theme(plot.title = element_text(family = \"Commissioner\", size = 21),\n                  plot.caption = element_text(family = \"Commissioner\", 30))\n  )\n\n\n\n\n\n\n\n\nOnly nine brazilian athletes maintained Gold Medals in the next Olympics edition after their first Gold Medal. They are:\n\ndf %&gt;%\n  filter(next_medal != \"no_participation\") %&gt;%\n  filter(country_noc == \"BRA\") %&gt;%\n  filter(next_medal == \"Gold\") %&gt;%\n  select(athlete, sport)\n\n# A tibble: 9 × 2\n  athlete          sport     \n  &lt;chr&gt;            &lt;chr&gt;     \n1 Adhemar da Silva Athletics \n2 Fabiana          Volleyball\n3 Paula            Volleyball\n4 Thaísa           Volleyball\n5 Jaque            Volleyball\n6 Sheilla Tavares  Volleyball\n7 Fabi             Volleyball\n8 Kahena Kunze     Sailing   \n9 Martine Grael    Sailing   \n\n\nThat is it for this small exercise. I tried to ask a very objective question, and answer it with good data :)"
  },
  {
    "objectID": "posts/post_2/post_2.html",
    "href": "posts/post_2/post_2.html",
    "title": "Premier League teams home and away",
    "section": "",
    "text": "library(tidyverse)\nlibrary(ggbump)\n\nsoccer &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2023/2023-04-04/soccer21-22.csv')\n\naway_vic &lt;- soccer %&gt;%\n  group_by(AwayTeam) %&gt;%\n  summarise(vic_away = sum(FTR == \"A\")) %&gt;%\n  arrange(desc(vic_away)) %&gt;%\n  rownames_to_column(var = \"vic_away_rank\") %&gt;%\n  rename(team = AwayTeam) %&gt;%\n  select(-vic_away)\n  \n\nhome_vic &lt;- soccer %&gt;%\n  group_by(HomeTeam) %&gt;%\n  summarise(vic_home = sum(FTR == \"H\")) %&gt;%\n  arrange(desc(vic_home)) %&gt;%\n  rownames_to_column(var = \"vic_home_rank\") %&gt;%\n  rename(team = HomeTeam) %&gt;%\n  select(-vic_home)\n\nvictories &lt;- merge(home_vic, away_vic, by = \"team\") %&gt;%\n  mutate(\n    vic_home_rank = as.numeric(vic_home_rank),\n    vic_away_rank = as.numeric(vic_away_rank)\n  ) %&gt;%\n  arrange(vic_home_rank) %&gt;%\n  pivot_longer(\n    cols = c(\"vic_home_rank\", \"vic_away_rank\"),\n    names_to = \"home_away\",\n    values_to = \"rank\"\n  ) %&gt;%\n  mutate(\n    home_away_enc = as.integer(fct_rev(factor(home_away)))\n  )\n\nvictories %&gt;%\n  ggplot(aes(home_away_enc, rank, col = team)) +\n  geom_point(shape = \"|\", stroke = 5) +\n  ggbump::geom_bump(linewidth = 1.5) +\n  geom_text(\n    data = victories %&gt;% filter(home_away_enc == 1),\n    aes(label = team),\n    hjust = 1,\n    nudge_x = -0.1,\n    size = 4,\n    fontface = \"bold\"\n  ) +\n  geom_text(\n    data = victories %&gt;% filter(home_away_enc == 2),\n    aes(label = team),\n    hjust = 0,\n    nudge_x = 0.1,\n    size = 4,\n    fontface = \"bold\"\n  ) +\n  scale_y_reverse(breaks = 1:20) +\n  scale_x_continuous(breaks = 1:2, labels = c(\"Victories Home Rank\", \"Victories Away Rank\")) +\n  coord_cartesian(xlim = c(0.5, 2.5)) +\n  theme_minimal() +\n  labs(\n    title = \"Best defenders vs best visitors in Premier League 21/22\",\n    caption = \"2023-04-04 Tidytuesday data\"\n  ) +\n  theme(\n    axis.title = element_blank(),\n    \n    plot.background = element_rect(fill = \"#f5ede4\", colour = NA),\n    plot.margin = margin(t = 2, b =4, l = 5, r = 5, unit = \"mm\"),\n    \n    panel.grid = element_blank(),\n    \n    legend.position = \"none\"\n  )"
  },
  {
    "objectID": "posts/post_2/post_2.html#premier-league-teams-home-and-away",
    "href": "posts/post_2/post_2.html#premier-league-teams-home-and-away",
    "title": "Premier League teams home and away",
    "section": "",
    "text": "library(tidyverse)\nlibrary(ggbump)\n\nsoccer &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2023/2023-04-04/soccer21-22.csv')\n\naway_vic &lt;- soccer %&gt;%\n  group_by(AwayTeam) %&gt;%\n  summarise(vic_away = sum(FTR == \"A\")) %&gt;%\n  arrange(desc(vic_away)) %&gt;%\n  rownames_to_column(var = \"vic_away_rank\") %&gt;%\n  rename(team = AwayTeam) %&gt;%\n  select(-vic_away)\n  \n\nhome_vic &lt;- soccer %&gt;%\n  group_by(HomeTeam) %&gt;%\n  summarise(vic_home = sum(FTR == \"H\")) %&gt;%\n  arrange(desc(vic_home)) %&gt;%\n  rownames_to_column(var = \"vic_home_rank\") %&gt;%\n  rename(team = HomeTeam) %&gt;%\n  select(-vic_home)\n\nvictories &lt;- merge(home_vic, away_vic, by = \"team\") %&gt;%\n  mutate(\n    vic_home_rank = as.numeric(vic_home_rank),\n    vic_away_rank = as.numeric(vic_away_rank)\n  ) %&gt;%\n  arrange(vic_home_rank) %&gt;%\n  pivot_longer(\n    cols = c(\"vic_home_rank\", \"vic_away_rank\"),\n    names_to = \"home_away\",\n    values_to = \"rank\"\n  ) %&gt;%\n  mutate(\n    home_away_enc = as.integer(fct_rev(factor(home_away)))\n  )\n\nvictories %&gt;%\n  ggplot(aes(home_away_enc, rank, col = team)) +\n  geom_point(shape = \"|\", stroke = 5) +\n  ggbump::geom_bump(linewidth = 1.5) +\n  geom_text(\n    data = victories %&gt;% filter(home_away_enc == 1),\n    aes(label = team),\n    hjust = 1,\n    nudge_x = -0.1,\n    size = 4,\n    fontface = \"bold\"\n  ) +\n  geom_text(\n    data = victories %&gt;% filter(home_away_enc == 2),\n    aes(label = team),\n    hjust = 0,\n    nudge_x = 0.1,\n    size = 4,\n    fontface = \"bold\"\n  ) +\n  scale_y_reverse(breaks = 1:20) +\n  scale_x_continuous(breaks = 1:2, labels = c(\"Victories Home Rank\", \"Victories Away Rank\")) +\n  coord_cartesian(xlim = c(0.5, 2.5)) +\n  theme_minimal() +\n  labs(\n    title = \"Best defenders vs best visitors in Premier League 21/22\",\n    caption = \"2023-04-04 Tidytuesday data\"\n  ) +\n  theme(\n    axis.title = element_blank(),\n    \n    plot.background = element_rect(fill = \"#f5ede4\", colour = NA),\n    plot.margin = margin(t = 2, b =4, l = 5, r = 5, unit = \"mm\"),\n    \n    panel.grid = element_blank(),\n    \n    legend.position = \"none\"\n  )"
  },
  {
    "objectID": "posts/post_0/post_0.html",
    "href": "posts/post_0/post_0.html",
    "title": "My Post",
    "section": "",
    "text": "Exercise from the incredible book “Analises Ecológicas no R”\n\nlibrary(ggpubr)\nlibrary(tidyverse)\nlibrary(ecodados)\n\nspider &lt;- Cap7_exercicio1\n\nt.test(Tamanho ~ Sexo, data = spider, var.equal = TRUE)\n\n\n    Two Sample t-test\n\ndata:  Tamanho by Sexo\nt = 2.2756, df = 28, p-value = 0.03072\nalternative hypothesis: true difference in means between group f and group m is not equal to 0\n95 percent confidence interval:\n 0.2356113 4.4843887\nsample estimates:\nmean in group f mean in group m \n          10.08            7.72 \n\n\n\nspider %&gt;%\n  ggplot(aes(x = Sexo, y = Tamanho, color = Sexo)) +\n  geom_boxplot(fill = c(\"darkorange\", \"cyan4\"), color = \"black\") +\n  geom_jitter(shape = 16, cex = 5, color = \"black\") +\n  scale_x_discrete(name = \"Sex\",\n                   labels = c(\"Female\", \"Male\")) +\n  scale_y_continuous(\n    name = \"Size\"\n  ) +\n  theme_bw() +\n  stat_compare_means(method = \"t.test\",\n                     label.x = 1.4,\n                     label.y = 14)"
  },
  {
    "objectID": "posts/post_0/post_0.html#spiders-sexual-dimorphism",
    "href": "posts/post_0/post_0.html#spiders-sexual-dimorphism",
    "title": "My Post",
    "section": "",
    "text": "Exercise from the incredible book “Analises Ecológicas no R”\n\nlibrary(ggpubr)\nlibrary(tidyverse)\nlibrary(ecodados)\n\nspider &lt;- Cap7_exercicio1\n\nt.test(Tamanho ~ Sexo, data = spider, var.equal = TRUE)\n\n\n    Two Sample t-test\n\ndata:  Tamanho by Sexo\nt = 2.2756, df = 28, p-value = 0.03072\nalternative hypothesis: true difference in means between group f and group m is not equal to 0\n95 percent confidence interval:\n 0.2356113 4.4843887\nsample estimates:\nmean in group f mean in group m \n          10.08            7.72 \n\n\n\nspider %&gt;%\n  ggplot(aes(x = Sexo, y = Tamanho, color = Sexo)) +\n  geom_boxplot(fill = c(\"darkorange\", \"cyan4\"), color = \"black\") +\n  geom_jitter(shape = 16, cex = 5, color = \"black\") +\n  scale_x_discrete(name = \"Sex\",\n                   labels = c(\"Female\", \"Male\")) +\n  scale_y_continuous(\n    name = \"Size\"\n  ) +\n  theme_bw() +\n  stat_compare_means(method = \"t.test\",\n                     label.x = 1.4,\n                     label.y = 14)"
  },
  {
    "objectID": "2_Blog.html",
    "href": "2_Blog.html",
    "title": "Portfolio",
    "section": "",
    "text": "In my GitHub pages I developed some small coding exercises. Most of these are applications of algorithms I have studied or projects from websites such as Kaggle or DataCamp.\nI structured these projects as Blog Posts, using the Quarto Blog strucure.\nMostly, these are some Data Storytelling examples!\n               \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPredicting number of points of NBA players in the regular season\n\n\n\n\n\nApplying machine learning to NBA data\n\n\n\n\n\nAug 30, 2024\n\n\nJoão Afonso Poester-Carvalho\n\n\n\n\n\n\n\n\n\n\n\n\nRecent ATP Rank 1 Men Tennis Players\n\n\n\n\n\nHow can we visualize the evolution of the ATP Rank 1 Men Tennis Players?\n\n\n\n\n\nAug 22, 2024\n\n\nJoão Afonso Poester-Carvalho\n\n\n\n\n\n\n\n\n\n\n\n\nHow do athletes perform in the next Olympics Edition after winning their first Gold Medal?\n\n\n\n\n\nAfter the FIRST GOLD MEDAL, do athletes maintain the same result in the enxt Olympics edition\n\n\n\n\n\nAug 4, 2024\n\n\nJoão Afonso Poester-Carvalho\n\n\n\n\n\n\n\n\n\n\n\n\nSpiders Sexual Dimorphism\n\n\n\n\n\nPost description\n\n\n\n\n\nAug 2, 2024\n\n\nJoão Afonso Poester-Carvalho\n\n\n\n\n\n\n\n\n\n\n\n\nBrazilian Gradute Courses\n\n\n\n\n\nPost description\n\n\n\n\n\nAug 2, 2024\n\n\nJoão Afonso Poester-Carvalho\n\n\n\n\n\n\n\n\n\n\n\n\nPremier League teams home and away\n\n\n\n\n\nPost description\n\n\n\n\n\nAug 2, 2024\n\n\nJoão Afonso Poester-Carvalho\n\n\n\n\n\n\n\n\n\n\n\n\nBoard games average rating\n\n\n\n\n\nPost description\n\n\n\n\n\nAug 2, 2024\n\n\nJoão Afonso Poester-Carvalho\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "1_MAIN.html",
    "href": "1_MAIN.html",
    "title": "João Afonso Poester-Carvalho",
    "section": "",
    "text": "Hello, my name is João Afonso Poester-Carvalho and I am graduated in Biological Sciences (Universidade Federal do Rio Grande - FURG). During my Graduation, I focused on Biogeography and species distribution patterns (e.g. Poester-Carvalho et al. 2023), with different taxa as study systems. I am able to use techniques such as Endemicity Analysis and Hierarchical Clustering to better understand species distribution patterns across the Neotropics. Currently, I am a Masters Student researching community ecology and ecophysiological resposes of dormant invertebrate stages.\nDuring much of my graduation, I focused on studying programming and GIS tools to solve problems associated with my work.\nRStats is one of my main interests of study. I am continuously trying to evolve my skills with Spatial Analysis and especially Statistical Analysis.\nI am much interested in studying Statistical and Machine Learning Models and their applications to ecological data, especially to conservation focus.\nCurrently, I have been trying to develop small projects with the skills I learn, such as analysis employing various Statistical Tools."
  },
  {
    "objectID": "1_MAIN.html#skills",
    "href": "1_MAIN.html#skills",
    "title": "João Afonso Poester-Carvalho",
    "section": "Skills",
    "text": "Skills\nI am especially interested and continuously developing my skills on:\n\nR Programming\n\nGeneral data wrangling problems - I particularly like this\nGeneral dataviz techniques (working on these in my GitHub page!)\nQuarto projects (such as this website!)\nInteractive reports (Reactable; Leaflet)\nIntegration with Git and Github (version control, etc.)\nDeploying websites with GitHub Pages (I am currently developing a publication with a dashboard for species distribution)\n\n\n       \n\nQGIS and R map building\n\nManipulation of spatial data in R\nUsing the “sf” package in R\n\n\n       \n\nStatistical techniques:\n\nInferential Analysis\n\nSimple and Multivariate Linear Models\nSimple and Multivariate Generalized Linear Models\n\nMultivariate Clustering and Ordination Analysis\n\nClustering Analysis (UPGMA; WPGMA) - I mastered these techniques during my Graduation to perform Biogeographical Regionalization\nDimensionality Reduction Analysis (PCA)\n\n\n\n       \n\nMachine Learning Algorithms - still beginning on this topic\n\n       \n\nCommunity Ecology, Macroecology and Biogeography\n\nSpecies Occurences Databases Cleaning (e.g. Ecology Datapapers)\nGBIF Data\nTrait Data\n\nEcological Niche Modelling\n\nCurrently developing myself on this topic, especially working with the “SDM” R package"
  },
  {
    "objectID": "1_Publications.html",
    "href": "1_Publications.html",
    "title": "Publications",
    "section": "",
    "text": "Articles\n\nPoester-Carvalho, J. A.; Barão, K.; Costa, L. G.; Ferrari, A. (2023). Areas of endemism and sampling bias of Pentatomidae (Heteroptera) in the Americas. Journal of Insect Conservation, 27(5), 781–794."
  },
  {
    "objectID": "posts/post_1/post_1.html",
    "href": "posts/post_1/post_1.html",
    "title": "Brazilian Gradute Courses",
    "section": "",
    "text": "library(tidyverse)\nlibrary(janitor)\n\nlibrary(tidyverse)\nlibrary(janitor)\n\n\ncursos &lt;- read.csv(\"cursos_graduacao.csv\")\ncursos &lt;- cursos %&gt;%\n  clean_names() %&gt;%\n  pivot_longer(!c(ano), names_to = \"modalidade\", values_to = \"n\") %&gt;%\n  as.data.frame()\n\ncursos %&gt;% \n  group_by(ano) %&gt;%\n  mutate(perc = proportions(n)) %&gt;%\n  ggplot(aes(x = ano, y = perc, fill = modalidade)) +\n    geom_area() +\n    theme_minimal() +\n    scale_x_continuous(\n      name = \"Ano\",\n      breaks = c(2012:2022)\n    ) +\n    scale_y_continuous(\n      name = \"Porcentagem\",\n      labels = scales::percent\n    ) +\n    scale_fill_brewer(\n      name = \"Modalidade\",\n      palette = \"Dark2\",\n      labels = c(\"Bacharelado Presencial\", \"Bacharelado Distância\", \"Licenciatura Presencial\", \"Licenciatura Distância\", \"Tecnológico Presencial\", \"Tecnológico Distância\")\n    ) +\n    labs(\n      caption = \"Dados: Censo do Ensino Superior 2022 - INEP\"\n    ) +\n    theme(\n      plot.background = element_rect(fill = \"grey\", colour = NA),\n      plot.caption = element_text(hjust = -0.25) \n    )"
  },
  {
    "objectID": "posts/post_1/post_1.html#brazilian-gradute-courses",
    "href": "posts/post_1/post_1.html#brazilian-gradute-courses",
    "title": "Brazilian Gradute Courses",
    "section": "",
    "text": "library(tidyverse)\nlibrary(janitor)\n\nlibrary(tidyverse)\nlibrary(janitor)\n\n\ncursos &lt;- read.csv(\"cursos_graduacao.csv\")\ncursos &lt;- cursos %&gt;%\n  clean_names() %&gt;%\n  pivot_longer(!c(ano), names_to = \"modalidade\", values_to = \"n\") %&gt;%\n  as.data.frame()\n\ncursos %&gt;% \n  group_by(ano) %&gt;%\n  mutate(perc = proportions(n)) %&gt;%\n  ggplot(aes(x = ano, y = perc, fill = modalidade)) +\n    geom_area() +\n    theme_minimal() +\n    scale_x_continuous(\n      name = \"Ano\",\n      breaks = c(2012:2022)\n    ) +\n    scale_y_continuous(\n      name = \"Porcentagem\",\n      labels = scales::percent\n    ) +\n    scale_fill_brewer(\n      name = \"Modalidade\",\n      palette = \"Dark2\",\n      labels = c(\"Bacharelado Presencial\", \"Bacharelado Distância\", \"Licenciatura Presencial\", \"Licenciatura Distância\", \"Tecnológico Presencial\", \"Tecnológico Distância\")\n    ) +\n    labs(\n      caption = \"Dados: Censo do Ensino Superior 2022 - INEP\"\n    ) +\n    theme(\n      plot.background = element_rect(fill = \"grey\", colour = NA),\n      plot.caption = element_text(hjust = -0.25) \n    )"
  },
  {
    "objectID": "posts/post_3/post_3.html",
    "href": "posts/post_3/post_3.html",
    "title": "Board games average rating",
    "section": "",
    "text": "library(tidyverse)\n\nboard_games &lt;- readr::read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-03-12/board_games.csv\")\n\nboard_games %&gt;%\n  mutate(\n    decade = case_when(\n      year_published &gt;= 1950 & year_published &lt;=1959 ~ 1950,\n      year_published &gt;= 1960 & year_published &lt;=1969 ~ 1960,\n      year_published &gt;= 1970 & year_published &lt;=1979 ~ 1970,\n      year_published &gt;= 1980 & year_published &lt;=1989 ~ 1980,\n      year_published &gt;= 1990 & year_published &lt;=1999 ~ 1990,\n      year_published &gt;= 2000 & year_published &lt;=2009 ~ 2000,\n      year_published &gt;= 2010 & year_published &lt;=2019 ~ 2010,\n      year_published &gt;= 2020 ~ 2020,\n    )\n  ) %&gt;%\n  ggplot(aes(x = average_rating, fill = factor(decade))) +\n    geom_density() +\n    scale_y_continuous(\n      name = element_blank(),\n      breaks = NULL\n    ) +\n    scale_x_continuous(\n      name = \"Average Rating\",\n      breaks = c(0:10)\n    ) +\n    scale_fill_brewer(\n      name = \"Decade\",\n      palette = \"Set2\"\n    ) +\n    facet_wrap(~decade, ncol = 1) +\n    theme_minimal() +\n    labs(\n      title = \"Average Rating of Board Games Across Decades\"\n    ) +\n    theme(\n      plot.background = element_blank(),\n      panel.grid = element_blank()\n    )"
  },
  {
    "objectID": "posts/post_3/post_3.html#board-games-average-rating",
    "href": "posts/post_3/post_3.html#board-games-average-rating",
    "title": "Board games average rating",
    "section": "",
    "text": "library(tidyverse)\n\nboard_games &lt;- readr::read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-03-12/board_games.csv\")\n\nboard_games %&gt;%\n  mutate(\n    decade = case_when(\n      year_published &gt;= 1950 & year_published &lt;=1959 ~ 1950,\n      year_published &gt;= 1960 & year_published &lt;=1969 ~ 1960,\n      year_published &gt;= 1970 & year_published &lt;=1979 ~ 1970,\n      year_published &gt;= 1980 & year_published &lt;=1989 ~ 1980,\n      year_published &gt;= 1990 & year_published &lt;=1999 ~ 1990,\n      year_published &gt;= 2000 & year_published &lt;=2009 ~ 2000,\n      year_published &gt;= 2010 & year_published &lt;=2019 ~ 2010,\n      year_published &gt;= 2020 ~ 2020,\n    )\n  ) %&gt;%\n  ggplot(aes(x = average_rating, fill = factor(decade))) +\n    geom_density() +\n    scale_y_continuous(\n      name = element_blank(),\n      breaks = NULL\n    ) +\n    scale_x_continuous(\n      name = \"Average Rating\",\n      breaks = c(0:10)\n    ) +\n    scale_fill_brewer(\n      name = \"Decade\",\n      palette = \"Set2\"\n    ) +\n    facet_wrap(~decade, ncol = 1) +\n    theme_minimal() +\n    labs(\n      title = \"Average Rating of Board Games Across Decades\"\n    ) +\n    theme(\n      plot.background = element_blank(),\n      panel.grid = element_blank()\n    )"
  },
  {
    "objectID": "posts/post_5/post_5.html",
    "href": "posts/post_5/post_5.html",
    "title": "Recent ATP Rank 1 Men Tennis Players",
    "section": "",
    "text": "Hi, this time I tried to analyse some Tennis data. I obtained a dataset of ATP points, and turned it into a dataset of Rank 1 players and the dates they were Rank 1.\n\nlibrary(tidyverse)\nlibrary(ggalt)\nlibrary(sysfonts)\nlibrary(showtext)\n\natp &lt;- read_csv(\"ATP_Rankings_1990-2019.csv\")\n\nfont_add_google(\"Merriweather Sans\", regular.wt = 400, family = \"merri\")\nshowtext_auto(enable = TRUE)\n\nNow let’s process the data to obtain what we want.\n\nrank_1 &lt;- atp %&gt;%\n  filter(Points &gt; 0) %&gt;%\n  mutate(Date = lubridate::as_date(Date))  %&gt;%\n  group_by(Date) %&gt;%\n  slice_max(Points) %&gt;%\n  arrange(Date) %&gt;%\n  ungroup() %&gt;%\n    mutate(Rank1_Change = Player != lag(Player, default = first(Player))) %&gt;% \n    group_by(Player) %&gt;%\n    mutate(Group = cumsum(Rank1_Change)) %&gt;%\n    group_by(Player, Group) %&gt;%\n    summarize(\n      First_Day = min(Date),\n      Last_Day = max(Date)\n    ) %&gt;%\n    ungroup() %&gt;%\n    select(-Group) \n\nrank_1\n\n# A tibble: 44 × 3\n   Player          First_Day  Last_Day  \n   &lt;chr&gt;           &lt;date&gt;     &lt;date&gt;    \n 1 Andre Agassi    1999-07-05 1999-07-19\n 2 Andre Agassi    1999-09-13 2000-09-04\n 3 Andre Agassi    2003-04-28 2003-05-05\n 4 Andre Agassi    2003-06-16 2003-09-01\n 5 Andy Murray     2016-11-07 2017-08-14\n 6 Andy Roddick    2003-11-03 2004-01-26\n 7 Carlos Moya     1999-03-15 1999-03-22\n 8 Gustavo Kuerten 2000-12-04 2001-01-22\n 9 Gustavo Kuerten 2001-02-26 2001-03-26\n10 Gustavo Kuerten 2001-04-23 2001-11-12\n# ℹ 34 more rows\n\n\nThese two objects will help with the plot.\n\nplayers &lt;- rank_1 %&gt;% \n  select(Player) %&gt;%\n  arrange(desc(Player)) %&gt;%\n  unique() %&gt;%\n  as.vector() %&gt;%\n  unlist() \n\nnames &lt;- rank_1 %&gt;%\n  group_by(Player) %&gt;%\n  slice_min(First_Day)\n\nNow, we make our plot, using a Dumbell chart!\n\nrank_1 %&gt;%\n  ggplot() +\n    geom_dumbbell(aes(x = First_Day, xend = Last_Day, y = factor(Player, players)), size = 0.5, \n                  colour_x = \"#7bc133\", colour_xend = \"#7bc133\", fill = \"#7bc133\", colour = \"#7bc133\") +\n    geom_text(data = names, \n              aes(x = First_Day, y = factor(Player, players), label = factor(Player, players), hjust = 1.2),\n              family = \"merri\", colour = \"white\", size = 7.5) +\n    scale_x_date(expand = c(0.3,0), breaks = seq(as.Date(\"1996-01-01\"), as.Date(\"2020-01-01\"), by = \"year\"),\n                 date_labels = \"%Y\") +\n    theme_minimal() +\n    theme(axis.text.y = element_blank(),\n          axis.title.y = element_blank(),\n          panel.grid = element_blank(),\n          axis.title.x = element_blank(),\n          axis.text.x = element_text(family = \"merri\", colour = \"white\", size = 13),\n          plot.background = element_rect(fill = \"black\", colour = \"black\"),\n          plot.title = element_text(family = \"merri\", colour = \"white\", size = 25)) +\n    labs(\n      title = \"Rank 1 Men Tennis Players from 1996 to 2019\",\n      caption = \"Source: Kaggle and ATP Tour\"\n    )"
  }
]