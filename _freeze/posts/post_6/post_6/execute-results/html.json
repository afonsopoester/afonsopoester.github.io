{
  "hash": "d3d8ef04eb702ff7457d569ce0804e58",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Predcting number of points of NBA players in the regular season\"\ndescription: \"Applying machine learning to NBA data\"\nauthor: \"João Afonso Poester-Carvalho\"\ndate: \"8/30/2024\"\n---\n\n\nHello, this time, I will try to develop a regression model with Tidymodels, a framework I've been studying recently. We will analyse NBA data from Kaggle. I developed much of this code adapting blog posts from [Julia Silge](https://juliasilge.com/), which have been helping me to understand the initial steps to tidy modelling. \n\n&nbsp;\n&nbsp;\n&nbsp;\n&nbsp;\n&nbsp;\n&nbsp;\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(tidymodels)\n\n## NBA ----\n\n## Predict the number of points of a player in the regular season \n\n\nnba <- read_csv(\"nba.csv\")\n\nnba_f <- nba %>%\n  filter(Season_type == \"Regular%20Season\") %>%\n  select(\n    PTS, year, PLAYER_ID, TEAM_ID, GP, MIN, FG_PCT, FG3_PCT, FT_PCT, OREB, DREB, AST, STL, BLK, TOV, PF\n  ) %>%\n  mutate(\n    PLAYER_ID = as.character(PLAYER_ID),\n    TEAM_ID = as.character(TEAM_ID),\n    year = as.numeric(substr(year, 1, 4))\n  ) \n\nglimpse(nba_f)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 6,259\nColumns: 16\n$ PTS       <dbl> 2280, 2133, 2036, 2023, 1920, 1903, 1786, 1577, 1562, 1560, …\n$ year      <dbl> 2012, 2012, 2012, 2012, 2012, 2012, 2012, 2012, 2012, 2012, …\n$ PLAYER_ID <chr> \"201142\", \"977\", \"2544\", \"201935\", \"2546\", \"201566\", \"201939…\n$ TEAM_ID   <chr> \"1610612760\", \"1610612747\", \"1610612748\", \"1610612745\", \"161…\n$ GP        <dbl> 81, 78, 76, 78, 67, 82, 78, 82, 82, 74, 82, 78, 69, 79, 82, …\n$ MIN       <dbl> 3119, 3013, 2877, 2985, 2482, 2861, 2983, 3076, 3167, 2790, …\n$ FG_PCT    <dbl> 0.510, 0.463, 0.565, 0.438, 0.449, 0.438, 0.451, 0.416, 0.42…\n$ FG3_PCT   <dbl> 0.416, 0.324, 0.406, 0.368, 0.379, 0.323, 0.453, 0.287, 0.36…\n$ FT_PCT    <dbl> 0.905, 0.839, 0.753, 0.851, 0.830, 0.800, 0.900, 0.773, 0.84…\n$ OREB      <dbl> 46, 66, 97, 62, 134, 111, 59, 45, 42, 175, 48, 29, 86, 218, …\n$ DREB      <dbl> 594, 367, 513, 317, 326, 317, 255, 271, 215, 495, 272, 203, …\n$ AST       <dbl> 374, 469, 551, 455, 171, 607, 539, 496, 531, 192, 204, 604, …\n$ STL       <dbl> 116, 106, 129, 142, 52, 145, 126, 169, 74, 62, 76, 75, 128, …\n$ BLK       <dbl> 105, 25, 67, 38, 32, 24, 12, 36, 19, 91, 24, 30, 56, 22, 31,…\n$ TOV       <dbl> 280, 287, 226, 295, 175, 273, 240, 254, 243, 143, 151, 218, …\n$ PF        <dbl> 143, 173, 110, 178, 205, 189, 198, 164, 172, 187, 173, 194, …\n```\n\n\n:::\n:::\n\n\n&nbsp;\n&nbsp;\n&nbsp;\n&nbsp;\n&nbsp;\n&nbsp;\n&nbsp;\n&nbsp;\n&nbsp;\n&nbsp;\n&nbsp;\n&nbsp;\n\nFirst step: use tidymodels functions to separate the train and test datasets and create a vfold object.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## Data Split ----\n\nset.seed(502)\nnba_split <- initial_split(nba_f, prop = 0.80, strata = PTS)\nnba_train <- training(nba_split)\nnba_test  <-  testing(nba_split)\n\nnba_folds <- nba_train %>%\n  vfold_cv(v = 5, repeats = 1, strata = PTS)\n```\n:::\n\n\n&nbsp;\n&nbsp;\n&nbsp;\n&nbsp;\n&nbsp;\n&nbsp;\n&nbsp;\n&nbsp;\n&nbsp;\n&nbsp;\n&nbsp;\n&nbsp;\n\nNext, we begin to build a recipe, begining with the formula. The variable PLAYER_ID was set as an ID. \nFinally, we normalize numeric variables and encode categorical variables. \n\n\n::: {.cell}\n\n```{.r .cell-code}\n## Model formula ----\nform <- as.formula(paste(\"PTS\",\" ~ \", \".\"))\n\n## Model recipe ----\n\nmod_recipe <- recipe(formula = form, data = nba_train) %>%\n  update_role(PLAYER_ID, new_role = \"id\") %>%\n  step_normalize(all_numeric_predictors()) %>%\n  step_dummy(all_predictors(), -all_numeric(), one_hot = F)\n\nmod_recipe_prep <- prep(mod_recipe, retain = T)\nmod_recipe_prep\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n── Recipe ──────────────────────────────────────────────────────────────────────\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n── Inputs \n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nNumber of variables by role\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\noutcome:    1\npredictor: 14\nid:         1\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n── Training information \n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nTraining data contained 5006 data points and no incomplete rows.\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n── Operations \n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n• Centering and scaling for: year, GP, MIN, FG_PCT, FG3_PCT, ... | Trained\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n• Dummy variables from: TEAM_ID | Trained\n```\n\n\n:::\n:::\n\n\n&nbsp;\n&nbsp;\n&nbsp;\n&nbsp;\n&nbsp;\n&nbsp;\n&nbsp;\n&nbsp;\n\nLet's look how the data is transformed when the recipe is applied:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmod_recipe_prep %>% bake(new_data = NULL) %>% glimpse()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 5,006\nColumns: 44\n$ year                <dbl> -1.671321, -1.671321, -1.671321, -1.671321, -1.671…\n$ PLAYER_ID           <fct> 101179, 203093, 2562, 2554, 203104, 2248, 1894, 20…\n$ GP                  <dbl> -0.39049839, -1.18655440, -0.39049839, -1.18655440…\n$ MIN                 <dbl> -0.7267063, -1.0084401, -0.7050344, -0.8760011, -0…\n$ FG_PCT              <dbl> -1.0738170, 0.1319478, -0.8180487, -0.8728562, -0.…\n$ FG3_PCT             <dbl> -0.15512765, -0.11022703, 0.12710481, 0.21049167, …\n$ FT_PCT              <dbl> 0.02876769, 0.29362197, -2.48502473, -0.16174154, …\n$ OREB                <dbl> -0.632882005, -0.362763041, -0.740929591, -0.77694…\n$ DREB                <dbl> -0.8697689, -0.8271258, -0.7418397, -0.8697689, -0…\n$ AST                 <dbl> -0.2847410, -0.7418568, -0.6078746, -0.7891447, -0…\n$ STL                 <dbl> -0.2947645, -0.7415845, -0.3585959, -0.7415845, -0…\n$ BLK                 <dbl> -0.6878534, -0.1227314, -0.6172132, -0.5818930, -0…\n$ TOV                 <dbl> -0.4810592, -0.8367129, -0.8197770, -0.7859052, -0…\n$ PF                  <dbl> -0.47434503, -1.07983655, -0.50461961, -0.89818910…\n$ PTS                 <dbl> 105, 104, 103, 100, 99, 96, 95, 95, 93, 91, 88, 87…\n$ TEAM_ID_X1610612738 <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ TEAM_ID_X1610612739 <dbl> 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ TEAM_ID_X1610612740 <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1,…\n$ TEAM_ID_X1610612741 <dbl> 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ TEAM_ID_X1610612742 <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ TEAM_ID_X1610612743 <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ TEAM_ID_X1610612744 <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ TEAM_ID_X1610612745 <dbl> 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ TEAM_ID_X1610612746 <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ TEAM_ID_X1610612747 <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ TEAM_ID_X1610612748 <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,…\n$ TEAM_ID_X1610612749 <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ TEAM_ID_X1610612750 <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ TEAM_ID_X1610612751 <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,…\n$ TEAM_ID_X1610612752 <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ TEAM_ID_X1610612753 <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ TEAM_ID_X1610612754 <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ TEAM_ID_X1610612755 <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ TEAM_ID_X1610612756 <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,…\n$ TEAM_ID_X1610612757 <dbl> 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ TEAM_ID_X1610612758 <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ TEAM_ID_X1610612759 <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ TEAM_ID_X1610612760 <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,…\n$ TEAM_ID_X1610612761 <dbl> 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ TEAM_ID_X1610612762 <dbl> 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ TEAM_ID_X1610612763 <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,…\n$ TEAM_ID_X1610612764 <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ TEAM_ID_X1610612765 <dbl> 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,…\n$ TEAM_ID_X1610612766 <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n```\n\n\n:::\n:::\n\n\n&nbsp;\n&nbsp;\n&nbsp;\n&nbsp;\n&nbsp;\n&nbsp;\n&nbsp;\n&nbsp;\n&nbsp;\n&nbsp;\n&nbsp;\n&nbsp;\n\nThen, we define the LASSO regression model with the \"glmnet\" package and add it to a workflow object, along with the recipe. We also create a tunning grid, which will have the parameter \"penalty\" as the parameter we want to tune when we fit the first model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## Define model ----\n\nreg_model <- linear_reg(penalty = tune(), mixture = 1) %>%\n  set_engine(\"glmnet\")\n\n## Grid ----\nlambda_grid <- grid_regular(penalty(), levels = 50)\n\n## Start workflow ----\n\nreg_wf <- \n  workflow() %>%\n  add_model(reg_model)  %>%\n  add_recipe(mod_recipe)\nreg_wf\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n2 Recipe Steps\n\n• step_normalize()\n• step_dummy()\n\n── Model ───────────────────────────────────────────────────────────────────────\nLinear Regression Model Specification (regression)\n\nMain Arguments:\n  penalty = tune()\n  mixture = 1\n\nComputational engine: glmnet \n```\n\n\n:::\n:::\n\n\n&nbsp;\n&nbsp;\n&nbsp;\n&nbsp;\n&nbsp;\n&nbsp;\n&nbsp;\n&nbsp;\n&nbsp;\n&nbsp;\n&nbsp;\n&nbsp;\n\nNow, we fit the model with the k fold cross validation scheme defined earlier. \nThe tune_grid() function receives the workflow, the ressampling scheme and the grid for the lambda parameter.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## Fit with Tune Grid ----\nlasso_grid <- tune_grid(\n  reg_wf, \n  resamples = nba_folds,\n  grid = lambda_grid,  \n  metrics = metric_set(rmse, mae, rsq)\n)\nlasso_grid\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# Tuning results\n# 5-fold cross-validation using stratification \n# A tibble: 5 × 4\n  splits              id    .metrics           .notes          \n  <list>              <chr> <list>             <list>          \n1 <split [4003/1003]> Fold1 <tibble [150 × 5]> <tibble [0 × 3]>\n2 <split [4004/1002]> Fold2 <tibble [150 × 5]> <tibble [0 × 3]>\n3 <split [4005/1001]> Fold3 <tibble [150 × 5]> <tibble [0 × 3]>\n4 <split [4006/1000]> Fold4 <tibble [150 × 5]> <tibble [0 × 3]>\n5 <split [4006/1000]> Fold5 <tibble [150 × 5]> <tibble [0 × 3]>\n```\n\n\n:::\n:::\n\n\n&nbsp;\n&nbsp;\n&nbsp;\n&nbsp;\n&nbsp;\n&nbsp;\n&nbsp;\n&nbsp;\n&nbsp;\n&nbsp;\n&nbsp;\n&nbsp;\n\nFor each fold, the tune grid object holds the metrics obtained and we can plot the metrics considering the varible penalty.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## Metrics ----\n\nlasso_grid %>%\n  collect_metrics() %>%\n  ggplot(aes(penalty, mean, color = .metric)) +\n  geom_errorbar(aes(ymin = mean - std_err, ymax = mean + std_err), alpha = 0.5) +\n  geom_line(size = 1.5) +\n  facet_wrap(~.metric, scales = \"free\") +\n  scale_x_log10() +\n  theme(legend.position = \"none\") +\n  theme_bw()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](post_6_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\n&nbsp;\n&nbsp;\n&nbsp;\n&nbsp;\n&nbsp;\n&nbsp;\n&nbsp;\n&nbsp;\n&nbsp;\n&nbsp;\n&nbsp;\n&nbsp;\n\nThe performance of the model seems to get very slightly better with the LASSO penalty. \nSo, we pull the best model from the tune grid object, based on the RMSE.\nThe finalize_workflow() function unites the original workflow with the best model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlowest_rmse <- lasso_grid %>%\n  select_best(metric = \"rmse\")\n\nfinal_lasso <- finalize_workflow(\n  reg_wf,\n  lowest_rmse\n)\n\nfinal_lasso\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n2 Recipe Steps\n\n• step_normalize()\n• step_dummy()\n\n── Model ───────────────────────────────────────────────────────────────────────\nLinear Regression Model Specification (regression)\n\nMain Arguments:\n  penalty = 1e-10\n  mixture = 1\n\nComputational engine: glmnet \n```\n\n\n:::\n:::\n\n\n&nbsp;\n&nbsp;\n&nbsp;\n&nbsp;\n&nbsp;\n&nbsp;\n&nbsp;\n&nbsp;\n&nbsp;\n&nbsp;\n&nbsp;\n&nbsp;\n\nWe then apply the model one last time to the training and testing data, using the last_fit() function. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nlast_fit_lasso <- last_fit(\n  final_lasso,\n  nba_split\n) %>%\n  collect_metrics()\n\nlast_fit_lasso\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  <chr>   <chr>          <dbl> <chr>               \n1 rmse    standard     126.    Preprocessor1_Model1\n2 rsq     standard       0.925 Preprocessor1_Model1\n```\n\n\n:::\n:::\n",
    "supporting": [
      "post_6_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}